"""
LightGBM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
1ë¶„ë´‰ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ í›„ ì €ì¥
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from download_data import load_daily_csv
from feature_engineer import prepare_ml_data
import random
from datetime import datetime, timedelta


def load_multiple_days(start_date, end_date, data_dir="data/daily", timeframe="1s", max_days=30):
    """
    ì—¬ëŸ¬ ë‚ ì§œì˜ ë°ì´í„° ë¡œë“œ
    
    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        timeframe: ì‹œê°„ë´‰ ('1s' ë˜ëŠ” '1m')
        max_days: ìµœëŒ€ ë¡œë“œ ì¼ìˆ˜
    
    Returns:
        DataFrame: ë³‘í•©ëœ ë°ì´í„°
    """
    start = datetime.strptime(start_date, "%Y%m%d")
    end = datetime.strptime(end_date, "%Y%m%d")
    
    all_days = []
    current = start
    while current <= end:
        all_days.append(current.strftime("%Y%m%d"))
        current += timedelta(days=1)
    
    # ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´)
    if len(all_days) > max_days:
        all_days = sorted(random.sample(all_days, max_days))
    
    timeframe_name = "1ì´ˆë´‰" if timeframe == "1s" else "1ë¶„ë´‰"
    print(f"\nğŸ“… {len(all_days)}ì¼ì¹˜ {timeframe_name} ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    dfs = []
    for i, date_str in enumerate(all_days, 1):
        df = load_daily_csv(date_str, data_dir, timeframe)
        if df is not None and len(df) > 0:
            # ì»¬ëŸ¼ ì´ë¦„ ë§¤í•‘
            df = df.rename(columns={
                'date_time_utc': 'timestamp',
                'acc_trade_volume': 'volume'
            })
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            df = df.sort_index()
            dfs.append(df)
            
            if i % 10 == 0:
                print(f"  [{i}/{len(all_days)}] ë¡œë“œ ì™„ë£Œ...")
    
    if not dfs:
        raise ValueError("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    merged_df = pd.concat(dfs, axis=0)
    merged_df = merged_df.sort_index()
    
    print(f"âœ… ì´ {len(merged_df):,}ê°œ {timeframe_name} ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    return merged_df


def train_lgb_model(X_train, y_train, X_val, y_val, use_smote=True, target_ratio=0.5):
    """
    LightGBM ëª¨ë¸ í•™ìŠµ (SMOTE ì˜¤ë²„ìƒ˜í”Œë§ + scale_pos_weight)
    
    Args:
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        use_smote: SMOTE ì‚¬ìš© ì—¬ë¶€
        target_ratio: SMOTE ëª©í‘œ ë¹„ìœ¨ (0.3 = ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì˜ 30%ë¡œ)
    
    Returns:
        LightGBM model
    """
    print("\nğŸ¤– LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # SMOTE ì˜¤ë²„ìƒ˜í”Œë§ (í•™ìŠµ ë°ì´í„°ì—ë§Œ ì ìš©)
    if use_smote:
        print(f"   âš™ï¸  SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì¤‘ (ëª©í‘œ ë¹„ìœ¨: {target_ratio:.1%})...")
        original_pos = y_train.sum()
        original_neg = len(y_train) - original_pos
        print(f"   - ì›ë³¸: ì–‘ì„±={original_pos:,}, ìŒì„±={original_neg:,} (ë¹„ìœ¨={original_pos/len(y_train):.2%})")
        
        smote = SMOTE(sampling_strategy=target_ratio, random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
        
        new_pos = y_train_resampled.sum()
        new_neg = len(y_train_resampled) - new_pos
        print(f"   - ìƒ˜í”Œë§ í›„: ì–‘ì„±={new_pos:,}, ìŒì„±={new_neg:,} (ë¹„ìœ¨={new_pos/len(y_train_resampled):.2%})")
        
        X_train = X_train_resampled
        y_train = y_train_resampled
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    # scale_pos_weight ê³„ì‚°
    scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()
    
    # íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'scale_pos_weight': scale_pos_weight,  # ë¶ˆê· í˜• ê°€ì¤‘ì¹˜
        'verbose': -1,
        'seed': 42
    }
    
    print(f"   - scale_pos_weight: {scale_pos_weight:.2f}")
    
    # í•™ìŠµ
    model = lgb.train(
        params,
        train_data,
        num_boost_round=200,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'val'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=20),
            lgb.log_evaluation(period=20)
        ]
    )
    
    return model


def evaluate_model(model, X, y, dataset_name="Test"):
    """
    ëª¨ë¸ í‰ê°€
    """
    y_pred_proba = model.predict(X)
    y_pred = (y_pred_proba >= 0.5).astype(int)
    
    print(f"\nğŸ“Š {dataset_name} í‰ê°€ ê²°ê³¼:")
    print(f"   - ROC-AUC: {roc_auc_score(y, y_pred_proba):.4f}")
    print(f"\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y, y_pred, target_names=['í•˜ë½/ìœ ì§€', 'ìƒìŠ¹']))
    
    print(f"\ní˜¼ë™ í–‰ë ¬:")
    cm = confusion_matrix(y, y_pred)
    print(f"   [[TN={cm[0,0]:,}, FP={cm[0,1]:,}],")
    print(f"    [FN={cm[1,0]:,}, TP={cm[1,1]:,}]]")
    
    return y_pred_proba


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 80)
    print("ğŸš€ ë¹„íŠ¸ì½”ì¸ ML ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ (1ë¶„ë´‰)
    df = load_multiple_days("20250101", "20250530", data_dir="data/daily_1m", timeframe="1m", max_days=30)
    
    # 2. íŠ¹ì§• & ë¼ë²¨ ìƒì„± (ìƒëŒ€ ë­í¬ ê¸°ë°˜)
    print("\nğŸ“Š íŠ¹ì§• & ë¼ë²¨ ìƒì„± ì¤‘...")
    X, y, feature_cols, df_with_features = prepare_ml_data(
        df,
        future_minutes=20,      # 20ë¶„ í›„ ì˜ˆì¸¡ (ë” ê¸´ ì‹œê°„ìœ¼ë¡œ ì•ˆì •ì  ì˜ˆì¸¡)
        use_rank=True,          # ìƒëŒ€ ë­í¬ ê¸°ë°˜
        rank_percentile=0.8     # ìƒìœ„ 20%ë¥¼ ìƒìŠ¹ìœ¼ë¡œ ë¼ë²¨ë§
    )
    
    # 3. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€)
    print("\nâœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
    
    # ì‹œê³„ì—´ì´ë¯€ë¡œ ëœë¤í•˜ê²Œ ì„ì§€ ì•Šê³  ìˆœì„œëŒ€ë¡œ ë¶„í• 
    train_size = int(len(X) * 0.7)
    val_size = int(len(X) * 0.15)
    
    X_train = X.iloc[:train_size]
    y_train = y.iloc[:train_size]
    
    X_val = X.iloc[train_size:train_size+val_size]
    y_val = y.iloc[train_size:train_size+val_size]
    
    X_test = X.iloc[train_size+val_size:]
    y_test = y.iloc[train_size+val_size:]
    
    print(f"   - í•™ìŠµ: {len(X_train):,}ê°œ")
    print(f"   - ê²€ì¦: {len(X_val):,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    model = train_lgb_model(X_train, y_train, X_val, y_val)
    
    # 5. í‰ê°€
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ëª¨ë¸ í‰ê°€")
    print("=" * 80)
    
    evaluate_model(model, X_train, y_train, "í•™ìŠµ ë°ì´í„°")
    evaluate_model(model, X_val, y_val, "ê²€ì¦ ë°ì´í„°")
    evaluate_model(model, X_test, y_test, "í…ŒìŠ¤íŠ¸ ë°ì´í„°")
    
    # 6. íŠ¹ì§• ì¤‘ìš”ë„
    print("\n" + "=" * 80)
    print("ğŸ” íŠ¹ì§• ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)")
    print("=" * 80)
    
    importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)
    
    for i, row in importance.head(10).iterrows():
        print(f"   {row['feature']:<25} {row['importance']:>10.0f}")
    
    # 7. ëª¨ë¸ ì €ì¥
    model_path = "model/lgb_model.pkl"
    joblib.dump({
        'model': model,
        'feature_cols': feature_cols,
        'version': '1.0',
        'train_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }, model_path)
    
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    print("\n" + "=" * 80)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    random.seed(42)
    np.random.seed(42)
    main()

