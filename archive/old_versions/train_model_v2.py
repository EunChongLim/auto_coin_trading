"""
ëª¨ë¸ v2.0: ë©€í‹° íƒ€ì„í”„ë ˆì„ + 3-Class Classification
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from download_data import load_daily_csv
from multi_timeframe_features import prepare_multi_timeframe_data
import random
from datetime import datetime, timedelta


def load_multiple_days_v2(start_date, end_date, data_dir="data/daily_1m", timeframe="1m", max_days=30):
    """
    ì—¬ëŸ¬ ë‚ ì§œì˜ 1ë¶„ë´‰ ë°ì´í„° ë¡œë“œ
    """
    start = datetime.strptime(start_date, "%Y%m%d")
    end = datetime.strptime(end_date, "%Y%m%d")
    
    all_days = []
    current = start
    while current <= end:
        all_days.append(current.strftime("%Y%m%d"))
        current += timedelta(days=1)
    
    # ëœë¤ ìƒ˜í”Œë§
    if len(all_days) > max_days:
        all_days = sorted(random.sample(all_days, max_days))
    
    print(f"\nğŸ“… {len(all_days)}ì¼ì¹˜ 1ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    dfs = []
    for i, date_str in enumerate(all_days, 1):
        df = load_daily_csv(date_str, data_dir, timeframe)
        if df is not None and len(df) > 0:
            # ì»¬ëŸ¼ ë§¤í•‘
            df = df.rename(columns={
                'date_time_utc': 'timestamp',
                'acc_trade_volume': 'volume'
            })
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            df = df.sort_index()
            dfs.append(df)
            
            if i % 10 == 0:
                print(f"  [{i}/{len(all_days)}] ë¡œë“œ ì™„ë£Œ...")
    
    if not dfs:
        raise ValueError("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    merged_df = pd.concat(dfs, axis=0)
    merged_df = merged_df.sort_index()
    
    print(f"âœ… ì´ {len(merged_df):,}ê°œ 1ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    return merged_df


def train_3class_model(X_train, y_train, X_val, y_val, use_smote=False):
    """
    3-Class LightGBM ëª¨ë¸ í•™ìŠµ
    """
    print("\nğŸ¤– LightGBM 3-Class ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # SMOTE ì˜¤ë²„ìƒ˜í”Œë§ (ì˜µì…˜)
    if use_smote:
        print(f"   âš™ï¸  SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì¤‘...")
        print(f"   - ì›ë³¸ ë¼ë²¨ ë¶„í¬:")
        for label in [0, 1, 2]:
            count = (y_train == label).sum()
            print(f"      {label}: {count:,}ê°œ ({count/len(y_train)*100:.1f}%)")
        
        smote = SMOTE(sampling_strategy='not majority', random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
        
        print(f"   - ìƒ˜í”Œë§ í›„ ë¼ë²¨ ë¶„í¬:")
        for label in [0, 1, 2]:
            count = (y_train_resampled == label).sum()
            print(f"      {label}: {count:,}ê°œ ({count/len(y_train_resampled)*100:.1f}%)")
        
        X_train = X_train_resampled
        y_train = y_train_resampled
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    # íŒŒë¼ë¯¸í„° ì„¤ì • (3-class multiclass)
    params = {
        'objective': 'multiclass',
        'num_class': 3,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': 42
    }
    
    # í•™ìŠµ
    model = lgb.train(
        params,
        train_data,
        num_boost_round=200,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'val'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=20),
            lgb.log_evaluation(period=20)
        ]
    )
    
    return model


def evaluate_3class_model(model, X, y, dataset_name="Test"):
    """
    3-Class ëª¨ë¸ í‰ê°€
    """
    y_pred = model.predict(X, num_iteration=model.best_iteration)
    y_pred_class = np.argmax(y_pred, axis=1)
    
    accuracy = accuracy_score(y, y_pred_class)
    
    print(f"\nğŸ“Š {dataset_name} ë°ì´í„° í‰ê°€ ê²°ê³¼:")
    print(f"   - Accuracy: {accuracy:.4f}")
    
    print(f"\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    target_names = ['í•˜ë½(0)', 'íš¡ë³´(1)', 'ìƒìŠ¹(2)']
    print(classification_report(y, y_pred_class, target_names=target_names, zero_division=0))
    
    print(f"\ní˜¼ë™ í–‰ë ¬:")
    cm = confusion_matrix(y, y_pred_class)
    print(f"   ì‹¤ì œ\\ì˜ˆì¸¡ |  í•˜ë½  |  íš¡ë³´  |  ìƒìŠ¹")
    print(f"   ---------|--------|--------|--------")
    print(f"   í•˜ë½(0)  | {cm[0][0]:6d} | {cm[0][1]:6d} | {cm[0][2]:6d}")
    print(f"   íš¡ë³´(1)  | {cm[1][0]:6d} | {cm[1][1]:6d} | {cm[1][2]:6d}")
    print(f"   ìƒìŠ¹(2)  | {cm[2][0]:6d} | {cm[2][1]:6d} | {cm[2][2]:6d}")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 80)
    print("ğŸš€ ë¹„íŠ¸ì½”ì¸ ML ëª¨ë¸ v2.0 í•™ìŠµ ì‹œì‘")
    print("   - ë©€í‹° íƒ€ì„í”„ë ˆì„ (1m, 5m, 15m, 60m)")
    print("   - 3-Class Classification (í•˜ë½, íš¡ë³´, ìƒìŠ¹)")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ (1ë¶„ë´‰)
    df = load_multiple_days_v2("20250101", "20250530", data_dir="data/daily_1m", timeframe="1m", max_days=30)
    
    # 2. ë©€í‹° íƒ€ì„í”„ë ˆì„ íŠ¹ì§• & 3-Class ë¼ë²¨ ìƒì„±
    print("\nğŸ“Š ë©€í‹° íƒ€ì„í”„ë ˆì„ íŠ¹ì§• & ë¼ë²¨ ìƒì„± ì¤‘...")
    X, y, feature_cols, df_with_features = prepare_multi_timeframe_data(
        df,
        future_minutes=20,      # 20ë¶„ í›„ ì˜ˆì¸¡
        down_threshold=-0.003,  # -0.3% í•˜ë½
        up_threshold=0.005      # +0.5% ìƒìŠ¹
    )
    
    # 3. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
    print("\nâœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
    
    # ì‹œê³„ì—´ ìˆœì„œ ìœ ì§€
    train_size = int(len(X) * 0.7)
    val_size = int(len(X) * 0.15)
    
    X_train = X.iloc[:train_size]
    y_train = y.iloc[:train_size]
    
    X_val = X.iloc[train_size:train_size+val_size]
    y_val = y.iloc[train_size:train_size+val_size]
    
    X_test = X.iloc[train_size+val_size:]
    y_test = y.iloc[train_size+val_size:]
    
    print(f"   - í•™ìŠµ: {len(X_train):,}ê°œ")
    print(f"   - ê²€ì¦: {len(X_val):,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    model = train_3class_model(X_train, y_train, X_val, y_val, use_smote=False)
    
    # 5. ëª¨ë¸ í‰ê°€
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ëª¨ë¸ í‰ê°€")
    print("=" * 80)
    
    evaluate_3class_model(model, X_train, y_train, "í•™ìŠµ")
    evaluate_3class_model(model, X_val, y_val, "ê²€ì¦")
    evaluate_3class_model(model, X_test, y_test, "í…ŒìŠ¤íŠ¸")
    
    # 6. íŠ¹ì§• ì¤‘ìš”ë„
    print("\n" + "=" * 80)
    print("ğŸ” íŠ¹ì§• ì¤‘ìš”ë„ (ìƒìœ„ 15ê°œ)")
    print("=" * 80)
    
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importance()
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(15).iterrows():
        print(f"   {row['feature']:30s} {row['importance']:10.0f}")
    
    # 7. ëª¨ë¸ ì €ì¥
    model_data = {
        'model': model,
        'feature_cols': feature_cols,
        'version': '2.0',
        'type': '3-class',
        'train_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    joblib.dump(model_data, "model/lgb_model_v2.pkl")
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model/lgb_model_v2.pkl")
    
    print("\n" + "=" * 80)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    random.seed(42)
    np.random.seed(42)
    main()

